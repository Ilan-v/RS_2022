{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_raw=pd.read_csv('data/train.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting the data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 3 positive items per user\n",
    "df_val = train_set_raw.groupby('UserID').sample(n=3, random_state=10)\n",
    "# take the rest of the data as validation set\n",
    "df_train = train_set_raw[~train_set_raw.index.isin(df_val.index)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in train set: 3705\n",
      "Number of items in validation set: 2468\n"
     ]
    }
   ],
   "source": [
    "items_list = list(train_set_raw['ItemID'].unique())\n",
    "train_items_list = list(df_train['ItemID'].unique())\n",
    "val_items_list = list(df_val['ItemID'].unique())\n",
    "print(f'Number of items in train set: {len(train_items_list)}')\n",
    "print(f'Number of items in validation set: {len(val_items_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in train set: 6040\n",
      "Number of users in validation set: 6040\n"
     ]
    }
   ],
   "source": [
    "users_list = list(train_set_raw['UserID'].unique())\n",
    "print(f'Number of users in train set: {len(users_list)}')\n",
    "print(f'Number of users in validation set: {len(df_val[\"UserID\"].unique())}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the train and validation data sets with negative and positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items_dict_train = create_user_items_dict(df_train)\n",
    "user_items_dict_val = create_user_items_dict(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_probability_dict = create_item_popularity_dict(train_set_raw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "train_negative_random = load_negative_samples(user_items_dict_train, items_list, 'train', 'random')\n",
    "train_negative_popularity = load_negative_samples(user_items_dict_train, items_list, 'train', 'popularity', item_probability_dict)\n",
    "# validation set\n",
    "val_negative_random = load_negative_samples(user_items_dict_val, items_list, 'validation', 'random')\n",
    "val_negative_popularity = load_negative_samples(user_items_dict_val, items_list, 'validation', 'popularity', item_probability_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_negative_popularity[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating datasets for training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('data/train_datasets/train_random.pkl', 'rb') as f:\n",
    "        df_random = pickle.load(f)\n",
    "except:\n",
    "    df_random = create_dataset(train_negative_random, df_train)\n",
    "    with open('data/train_datasets/train_random.pkl', 'wb') as f:\n",
    "        pickle.dump(df_random, f)\n",
    "\n",
    "try:\n",
    "    with open('data/train_datasets/train_popularity.pkl', 'rb') as f:\n",
    "        df_popularity = pickle.load(f)\n",
    "except:\n",
    "    df_popularity = create_dataset(train_negative_popularity, df_train)\n",
    "    with open('data/train_datasets/train_popularity.pkl', 'wb') as f:\n",
    "        pickle.dump(df_popularity, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1939105/1939105 [00:53<00:00, 36461.01it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 5412.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.658 validation loss: 0.918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1939105/1939105 [00:52<00:00, 36811.42it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 5934.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss: 0.606 validation loss: 1.415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1939105/1939105 [00:52<00:00, 36975.35it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 5811.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss: 0.61 validation loss: 5.966\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1939105/1939105 [00:52<00:00, 36887.13it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 5071.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train loss: 0.651 validation loss: 11.629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1939105/1939105 [00:54<00:00, 35498.65it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 4705.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train loss: 0.793 validation loss: 13.267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1939105/1939105 [00:54<00:00, 35382.11it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 4743.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train loss: 29.396 validation loss: 137.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:  46%|████▌     | 896466/1939105 [00:25<00:29, 34929.36it/s]"
     ]
    }
   ],
   "source": [
    "radnom_users_embeddings, random_items_embeddings = training_loop( \n",
    "                                                    df_random,\n",
    "                                                    user_items_dict_val, val_negative_random,\n",
    "                                                    users_list, items_list,\n",
    "                                                    alpha_item = 1e-4,\n",
    "                                                    alpha_user = 1e-4,\n",
    "                                                    item_init_noise=0.1,\n",
    "                                                    user_init_noise=0.1,\n",
    "                                                    epochs = 10,\n",
    "                                                    k = 32,\n",
    "                                                    lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1939105/1939105 [00:56<00:00, 34488.86it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 4949.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.724 validation loss: 1.386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1939105/1939105 [00:56<00:00, 34458.73it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 4964.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss: 0.72 validation loss: 1.332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1939105/1939105 [00:56<00:00, 34601.03it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 5322.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss: 0.689 validation loss: 1.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1939105/1939105 [00:55<00:00, 35050.05it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 4982.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train loss: 0.661 validation loss: 1.087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1939105/1939105 [00:56<00:00, 34422.61it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 4879.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train loss: 0.649 validation loss: 1.034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1939105/1939105 [00:56<00:00, 34324.67it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 5006.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train loss: 0.64 validation loss: 0.992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1939105/1939105 [00:56<00:00, 34550.25it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 4599.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train loss: 0.633 validation loss: 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1939105/1939105 [00:55<00:00, 34728.47it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 5089.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train loss: 0.627 validation loss: 0.929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1939105/1939105 [00:56<00:00, 34611.41it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 4922.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train loss: 0.621 validation loss: 0.905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 1939105/1939105 [00:56<00:00, 34414.57it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 4981.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train loss: 0.616 validation loss: 0.885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "popularity_users_embeddings, popularity_items_embeddings = training_loop( \n",
    "                                                    df_popularity,\n",
    "                                                    user_items_dict_val, val_negative_popularity,\n",
    "                                                    users_list, items_list,\n",
    "                                                    alpha_item = 1e-4,\n",
    "                                                    alpha_user = 1e-4,\n",
    "                                                    item_init_noise=0.1,\n",
    "                                                    user_init_noise=0.1,\n",
    "                                                    epochs = 10,\n",
    "                                                    k = 32,\n",
    "                                                    lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hit_Rate_at_k(positive_samples:dict, negative_samples:dict, users_embeddings:dict, items_embeddings:dict, k):\n",
    "    \"\"\"\n",
    "    Calculate average hitrate@k for a given set of positive and negative samples per user.\n",
    "    Args:\n",
    "        positive_samples (dict): dictionary of positive samples per user\n",
    "        negative_samples (dict): dictionary of negative samples per user\n",
    "        users_embeddings (dict): dictionary of user embeddings\n",
    "        items_embeddings (dict): dictionary of item embeddings\n",
    "        k (int): number of items to consider for hitrate calculation\n",
    "    \"\"\"\n",
    "    hit_rate = 0\n",
    "    for user in tqdm(positive_samples.keys()):\n",
    "        user_hit_rate=0\n",
    "        items_score=[]\n",
    "        for item in positive_samples[user]:\n",
    "            positive_score = np.dot(users_embeddings[user], items_embeddings[item])\n",
    "            items_score.append((positive_score,1))\n",
    "        \n",
    "        negative_scores = [np.dot(users_embeddings[user], items_embeddings[item]) for item in negative_samples[user]]\n",
    "        negative_scores = [(score,0) for score in negative_scores]\n",
    "        items_score.extend(negative_scores)\n",
    "        items_score = sorted(items_score, key=lambda x: x[0], reverse=True)\n",
    "        items_score = items_score[:k]\n",
    "        user_hit_rate = sum([x[1] for x in items_score])\n",
    "        user_hit_rate = user_hit_rate/len(positive_samples[user])\n",
    "        hit_rate+=user_hit_rate\n",
    "    hit_rate = hit_rate/len(positive_samples.keys())\n",
    "    return hit_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [00:01<00:00, 4675.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: Hit rate at 10: 0.10104856512141253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [00:01<00:00, 4939.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit rate at 10: 0.5788631346578321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "base_user_emb = create_embeddings(users_list, 0.1,32)\n",
    "base_item_emb = create_embeddings(items_list, 0.1,32)\n",
    "hitrate_10_base = Hit_Rate_at_k(user_items_dict_val, val_negative_popularity, base_user_emb, base_item_emb, k)\n",
    "print(f'Baseline: Hit rate at {k}: {hitrate_10_base}')\n",
    "hitrate_10_trained = Hit_Rate_at_k(user_items_dict_val, val_negative_popularity, popularity_users_embeddings, popularity_items_embeddings, k)\n",
    "print(f'Hit rate at {k}: {hitrate_10_trained}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "base_user_emb = create_embeddings(users_list, 0.1,16)\n",
    "base_item_emb = create_embeddings(items_list, 0.1,16)\n",
    "hitrate_10_base = Hit_Rate_at_k(user_items_dict_val, val_negative_random, base_user_emb, base_item_emb, k)\n",
    "print(f'Hit rate at {k}: {hitrate_10_base}')\n",
    "hitrate_10_trained = Hit_Rate_at_k(user_items_dict_val, val_negative_random, radnom_users_embeddings, random_items_embeddings, k)\n",
    "print(f'Hit rate at {k}: {hitrate_10_trained}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MPR_calculation(positive_samples:dict, negative_samples:dict, users_embeddings:dict, items_embeddings:dict)->float:\n",
    "    MPR = 0\n",
    "    for user in tqdm(positive_samples.keys(), desc='MPR calculation'):\n",
    "        user_mpr=0\n",
    "        for item in positive_samples[user]:\n",
    "            positive_score = np.dot(users_embeddings[user], items_embeddings[item])\n",
    "            negative_scores = [np.dot(users_embeddings[user], items_embeddings[item]) for item in negative_samples[user]]\n",
    "            neg_lst = [(x,0) for x in negative_scores]\n",
    "            scores =  neg_lst + [(positive_score,1)]\n",
    "            #add positive score to the list of negative scores, sort the list and find the index of the positive score\n",
    "            scores = sorted(scores, key=lambda x: x[0], reverse=True)\n",
    "            for i in range(len(scores)):\n",
    "                rating = scores[i][1]\n",
    "                user_mpr += rating*(i+1)/len(scores)\n",
    "\n",
    "        MPR+=user_mpr/len(positive_samples[user])\n",
    "    MPR = MPR/len(positive_samples.keys())\n",
    "    return MPR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MPR calculation: 100%|██████████| 6040/6040 [00:03<00:00, 1633.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPR: 0.5013188719196281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MPR calculation: 100%|██████████| 6040/6040 [00:03<00:00, 1621.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPR: 0.12622370140108344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#MPR calc\n",
    "MPR_base = MPR_calculation(user_items_dict_val, val_negative_popularity, base_user_emb, base_item_emb)\n",
    "print(f'MPR: {MPR_base}')\n",
    "MPR_trained = MPR_calculation(user_items_dict_val, val_negative_popularity, popularity_users_embeddings, popularity_items_embeddings)\n",
    "print(f'MPR: {MPR_trained}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.760289930897027"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MPR_trained/MPR_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_on_test_set(row:pd.Series, users_embeddings:dict, items_embeddings:dict)->pd.Series:\n",
    "    user = row['UserID']\n",
    "    item_1 = row['Item1']\n",
    "    item_2 = row['Item2']\n",
    "\n",
    "    item_1_score = np.dot(users_embeddings[user], items_embeddings[item_1])\n",
    "    item_2_score = np.dot(users_embeddings[user], items_embeddings[item_2])\n",
    "\n",
    "    if item_1_score > item_2_score:\n",
    "        row['prediction'] = 0\n",
    "    else:\n",
    "        row['prediction'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilanv\\AppData\\Local\\Temp/ipykernel_44568/2933082444.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0.01,0.01,0.01])\n",
    "# check if all values in array are equal to 0\n",
    "np.all(a == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(a,0,atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.allclose(users_embeddings[user], 0, atol=1e-8):\n",
    "    print('user', user)\n",
    "if np.allclose(items_embeddings[item], 0, atol=1e-8): \n",
    "    print('item', item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_score = np.array([4,2,3,1,6])\n",
    "negative_scores = np.array([7,2,3,4,5])\n",
    "a = [(x,1) for x in positive_score]\n",
    "b = [(x,0) for x in negative_scores]\n",
    "a.extend(b)\n",
    "# scores = np.append(positive_score, negative_scores)\n",
    "# scores\n",
    "sorted_scores = sorted(a, key=lambda x: x[0], reverse=True)\n",
    "    mpr = 0 \n",
    "    for i in range(len(sorted_scores)):\n",
    "        rating = sorted_scores[i][1]\n",
    "        mpr += rating*(i+1)/len(sorted_scores)\n",
    "    mpr/len(positive_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3] + [4,5,6]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0d24260c26781aab6a6247b3ae992ece4f26fd75ea3713b7a84756a27d5e272"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
