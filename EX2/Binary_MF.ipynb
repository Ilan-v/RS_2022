{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_raw=pd.read_csv('data/train.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting the data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 3 positive items per user\n",
    "df_val = train_set_raw.groupby('UserID').sample(n=3, random_state=10)\n",
    "# take the rest of the data as validation set\n",
    "df_train = train_set_raw[~train_set_raw.index.isin(df_val.index)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in train set: 3705\n",
      "Number of items in validation set: 2468\n"
     ]
    }
   ],
   "source": [
    "items_list = list(train_set_raw['ItemID'].unique())\n",
    "train_items_list = list(df_train['ItemID'].unique())\n",
    "val_items_list = list(df_val['ItemID'].unique())\n",
    "print(f'Number of items in train set: {len(train_items_list)}')\n",
    "print(f'Number of items in validation set: {len(val_items_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in train set: 6040\n",
      "Number of users in validation set: 6040\n"
     ]
    }
   ],
   "source": [
    "users_list = list(train_set_raw['UserID'].unique())\n",
    "print(f'Number of users in train set: {len(users_list)}')\n",
    "print(f'Number of users in validation set: {len(df_val[\"UserID\"].unique())}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the train and validation data sets with negative and positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items_dict_train = create_user_items_dict(df_train)\n",
    "user_items_dict_val = create_user_items_dict(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_probability_dict = create_item_popularity_dict(train_set_raw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "train_negative_random = load_negative_samples(user_items_dict_train, items_list, 'train', 'random')\n",
    "train_negative_popularity = load_negative_samples(user_items_dict_train, items_list, 'train', 'popularity', item_probability_dict)\n",
    "# validation set\n",
    "val_negative_random = load_negative_samples(user_items_dict_val, items_list, 'validation', 'random')\n",
    "val_negative_popularity = load_negative_samples(user_items_dict_val, items_list, 'validation', 'popularity', item_probability_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_regularization(user_embs, item_embs, alpha_item, alpha_user):\n",
    "    \"\"\"\n",
    "    Calculate regularization loss for user and item embeddings.\n",
    "    Args:\n",
    "        user_embs (dict): dictionary of user embeddings\n",
    "        item_embs (dict): dictionary of item embeddings\n",
    "        reg_lambda (float): regularization parameter\n",
    "    \"\"\"\n",
    "    user_reg = 0\n",
    "    item_reg = 0\n",
    "    for user in user_embs:\n",
    "        user_reg += np.linalg.norm(user_embs[user])\n",
    "    for item in item_embs:\n",
    "        item_reg += np.sum(item_embs[item]**2)\n",
    "    return (alpha_user/2) * user_reg + (alpha_item/2) * item_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_log_loss(positive_samples: dict,\n",
    "                        negative_samples: dict,\n",
    "                        user_embeddings: dict,\n",
    "                        item_embeddings: dict,\n",
    "                        )-> float:\n",
    "    \"\"\"\n",
    "    Calculate log loss for a given set of positive and negative samples per user.\n",
    "    Args:\n",
    "        positive_samples (dict): dictionary of positive samples per user\n",
    "        negative_samples (dict): dictionary of negative samples per user\n",
    "        user_vectors (dict): dictionary of user vectors\n",
    "        item_vectors (dict): dictionary of item vectors\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for user in tqdm(positive_samples):\n",
    "        # get user vector\n",
    "        user_vector = user_embeddings[user]\n",
    "        # get positive and negative items vectors \n",
    "        pos_item_vectors = [item_embeddings[x] for x in positive_samples[user]]\n",
    "        neg_item_vectors = [item_embeddings[x] for x in negative_samples[user]]\n",
    "        # convert lists of arrays to matrices\n",
    "        pos_items_matrix = np.vstack(pos_item_vectors)\n",
    "        neg_items_matrix = np.vstack(neg_item_vectors)\n",
    "\n",
    "        # calculate loss for positive items\n",
    "        pos_loss = np.log(np.array([sigmoid(x) for x in np.dot(user_vector, pos_items_matrix.T)]))\n",
    "        # calculate loss for negative item\n",
    "        neg_loss = np.log(1 - np.array([sigmoid(x) for x in np.dot(user_vector, pos_items_matrix.T)]))\n",
    "        # add up losses\n",
    "        loss += np.sum(pos_loss)/len(pos_loss) + np.sum(neg_loss)/len(neg_loss)\n",
    "    \n",
    "    log_loss = -loss/(len(positive_samples))\n",
    "    return log_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loss_func(  positive_samples,\n",
    "                            negative_samples,\n",
    "                            user_embeddings,\n",
    "                            item_embeddings,\n",
    "                            alpha_user,\n",
    "                            alpha_item)-> float:\n",
    "    log_loss = validation_log_loss(positive_samples, negative_samples, user_embeddings, item_embeddings)\n",
    "    reg = validation_regularization(user_embeddings, item_embeddings, alpha_user, alpha_item)\n",
    "    return log_loss + reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loss_func(prediction: float,\n",
    "                    rating:int,\n",
    "                    user_embedding:np.ndarray,\n",
    "                    item_embedding:np.ndarray,\n",
    "                    alpha_item:float,\n",
    "                    alpha_user:float\n",
    "                    )-> float:\n",
    "    \"\"\"\n",
    "    Calculate loss for a given prediction, rating, user embedding and item embedding.\n",
    "    Args:\n",
    "        prediction (float): prediction for a given user-item pair\n",
    "        rating (int): actual rating for a given user-item pair\n",
    "        user_embedding (np.ndarray): user embedding\n",
    "        item_embedding (np.ndarray): item embedding\n",
    "    \"\"\"\n",
    "    log_loss =  rating * np.log(prediction) + (1 - rating) * np.log(1 -prediction)\n",
    "    regularization = (alpha_user/2) * np.linalg.norm(user_embedding) + (alpha_item/2) * np.linalg.norm(item_embedding)\n",
    "    return log_loss + regularization "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating datasets for training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [01:27<00:00, 68.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1939105, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_dataset(train_negative_random, df_train)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(  train_df: pd.DataFrame,\n",
    "                    user_items_dict_validation: dict,\n",
    "                    negative_samples_validation: dict,\n",
    "                    user_list: list,\n",
    "                    items_list: list,\n",
    "                    alpha_item: float,\n",
    "                    alpha_user: float,\n",
    "                    epochs: int,\n",
    "                    k: int,\n",
    "                    lr: float,\n",
    "                    ) -> tuple:\n",
    "    \n",
    "    items_embeddings = create_embeddings(items_list, alpha_item, k)\n",
    "    users_embeddings = create_embeddings(user_list, alpha_user, k)\n",
    "    train = train_df.values\n",
    "    for e in range(epochs):\n",
    "        np.random.shuffle(train)\n",
    "        loss = 0\n",
    "        for user, item, rating in tqdm(train, desc=f'Epoch {e+1}'):\n",
    "            prediction = sigmoid(np.dot(users_embeddings[user], items_embeddings[item]))\n",
    "            error = rating - prediction\n",
    "            users_embeddings[user] += lr * (error * items_embeddings[item] - alpha_user * users_embeddings[user])\n",
    "            items_embeddings[item] += lr * (error * users_embeddings[user] - alpha_item * items_embeddings[item])\n",
    "            # calculate loss\n",
    "            loss += train_loss_func(prediction, rating, users_embeddings[user], items_embeddings[item], alpha_item, alpha_user)\n",
    "        print(f'Epoch {e+1} loss: {-loss/len(train)}')\n",
    "    return users_embeddings, items_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1939105/1939105 [01:05<00:00, 29414.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.5976826048234564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1939105/1939105 [01:06<00:00, 29240.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 0.4531605242126664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "users_embeddings, items_embeddings = training_loop( df,\n",
    "                                                    user_items_dict_val, val_negative_random,\n",
    "                                                    users_list, items_list,\n",
    "                                                    alpha_item = 0.0001,\n",
    "                                                    alpha_user = 0.0001,\n",
    "                                                    epochs = 2,\n",
    "                                                    k = 16,\n",
    "                                                    lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_on_test_set(row:pd.Series, users_embeddings:dict, items_embeddings:dict)->pd.Series:\n",
    "    user = row['UserID']\n",
    "    item_1 = row['Item1']\n",
    "    item_2 = row['Item2']\n",
    "\n",
    "    item_1_score = np.dot(users_embeddings[user], items_embeddings[item_1])\n",
    "    item_2_score = np.dot(users_embeddings[user], items_embeddings[item_2])\n",
    "\n",
    "    if item_1_score > item_2_score:\n",
    "        row['prediction'] = 0\n",
    "    else:\n",
    "        row['prediction'] = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0d24260c26781aab6a6247b3ae992ece4f26fd75ea3713b7a84756a27d5e272"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
