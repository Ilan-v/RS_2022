{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_raw=pd.read_csv('data/train.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting the data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 3 positive items per user\n",
    "df_val = train_set_raw.groupby('UserID').sample(n=3, random_state=10)\n",
    "# take the rest of the data as validation set\n",
    "df_train = train_set_raw[~train_set_raw.index.isin(df_val.index)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in train set: 3705\n",
      "Number of items in validation set: 2468\n"
     ]
    }
   ],
   "source": [
    "items_list = list(train_set_raw['ItemID'].unique())\n",
    "train_items_list = list(df_train['ItemID'].unique())\n",
    "val_items_list = list(df_val['ItemID'].unique())\n",
    "print(f'Number of items in train set: {len(train_items_list)}')\n",
    "print(f'Number of items in validation set: {len(val_items_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users in train set: 6040\n",
      "Number of users in validation set: 6040\n"
     ]
    }
   ],
   "source": [
    "users_list = list(train_set_raw['UserID'].unique())\n",
    "print(f'Number of users in train set: {len(users_list)}')\n",
    "print(f'Number of users in validation set: {len(df_val[\"UserID\"].unique())}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the train and validation data sets with negative and positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items_dict_train = create_user_items_dict(df_train)\n",
    "user_items_dict_val = create_user_items_dict(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_probability_dict = create_item_popularity_dict(train_set_raw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "train_negative_random = load_negative_samples(user_items_dict_train, items_list, 'train', 'random')\n",
    "train_negative_popularity = load_negative_samples(user_items_dict_train, items_list, 'train', 'popularity', item_probability_dict)\n",
    "# validation set\n",
    "val_negative_random = load_negative_samples(user_items_dict_val, items_list, 'validation', 'random')\n",
    "val_negative_popularity = load_negative_samples(user_items_dict_val, items_list, 'validation', 'popularity', item_probability_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating datasets for training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [01:13<00:00, 81.85it/s] \n",
      "100%|██████████| 6040/6040 [01:10<00:00, 86.12it/s] \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('data/train_datasets/train_random.pkl', 'rb') as f:\n",
    "        df_random = pickle.load(f)\n",
    "except:\n",
    "    df_random = create_dataset(train_negative_random, df_train)\n",
    "    with open('data/train_datasets/train_random.pkl', 'wb') as f:\n",
    "        pickle.dump(df_random, f)\n",
    "\n",
    "try:\n",
    "    with open('data/train_datasets/train_popularity.pkl', 'rb') as f:\n",
    "        df_popularity = pickle.load(f)\n",
    "except:\n",
    "    df_popularity = create_dataset(train_negative_popularity, df_train)\n",
    "    with open('data/train_datasets/train_popularity.pkl', 'wb') as f:\n",
    "        pickle.dump(df_popularity, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radnom_users_embeddings, random_items_embeddings = training_loop( \n",
    "                                                    df_random,\n",
    "                                                    user_items_dict_val, val_negative_random,\n",
    "                                                    users_list, items_list,\n",
    "                                                    alpha_item = 1e-4,\n",
    "                                                    alpha_user = 1e-4,\n",
    "                                                    item_init_noise=0.1,\n",
    "                                                    user_init_noise=0.1,\n",
    "                                                    epochs = 10,\n",
    "                                                    k = 32,\n",
    "                                                    lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity_users_embeddings, popularity_items_embeddings = training_loop( \n",
    "                                                    df_popularity,\n",
    "                                                    user_items_dict_val, val_negative_popularity,\n",
    "                                                    users_list, items_list,\n",
    "                                                    alpha_item = 1e-4,\n",
    "                                                    alpha_user = 1e-4,\n",
    "                                                    item_init_noise=0.1,\n",
    "                                                    user_init_noise=0.1,\n",
    "                                                    epochs = 10,\n",
    "                                                    k = 32,\n",
    "                                                    lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dic = {}\n",
    "for sample_type in ['random', 'popularity']:\n",
    "    for emb in ['users','items']:\n",
    "        with open(f'data/results/{sample_type}_{emb}_embeddings.pkl', 'rb') as f:\n",
    "            res_dic[f'{sample_type}_{emb}'] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating measures on model trained on random using random as validation set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MPR calculation: 100%|██████████| 6040/6040 [00:03<00:00, 1923.91it/s]\n",
      "Hit Rate @1 calculation: 100%|██████████| 6040/6040 [00:00<00:00, 6144.92it/s]\n",
      "Hit Rate @10 calculation: 100%|██████████| 6040/6040 [00:00<00:00, 6174.62it/s]\n",
      "Hit Rate @50 calculation: 100%|██████████| 6040/6040 [00:01<00:00, 5692.97it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 5880.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating measures on model trained on random using popularity as validation set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MPR calculation: 100%|██████████| 6040/6040 [00:03<00:00, 1929.50it/s]\n",
      "Hit Rate @1 calculation: 100%|██████████| 6040/6040 [00:00<00:00, 6110.07it/s]\n",
      "Hit Rate @10 calculation: 100%|██████████| 6040/6040 [00:01<00:00, 5829.02it/s]\n",
      "Hit Rate @50 calculation: 100%|██████████| 6040/6040 [00:01<00:00, 5931.96it/s]\n",
      "100%|██████████| 6040/6040 [00:00<00:00, 6077.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating measures on model trained on popularity using random as validation set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MPR calculation: 100%|██████████| 6040/6040 [00:03<00:00, 1919.38it/s]\n",
      "Hit Rate @1 calculation: 100%|██████████| 6040/6040 [00:00<00:00, 6126.39it/s]\n",
      "Hit Rate @10 calculation: 100%|██████████| 6040/6040 [00:00<00:00, 6203.70it/s]\n",
      "Hit Rate @50 calculation: 100%|██████████| 6040/6040 [00:01<00:00, 5733.72it/s]\n",
      "100%|██████████| 6040/6040 [00:01<00:00, 5814.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating measures on model trained on popularity using popularity as validation set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MPR calculation: 100%|██████████| 6040/6040 [00:03<00:00, 1971.91it/s]\n",
      "Hit Rate @1 calculation: 100%|██████████| 6040/6040 [00:01<00:00, 5979.09it/s]\n",
      "Hit Rate @10 calculation: 100%|██████████| 6040/6040 [00:01<00:00, 5580.35it/s]\n",
      "Hit Rate @50 calculation: 100%|██████████| 6040/6040 [00:01<00:00, 5912.03it/s]\n",
      "100%|██████████| 6040/6040 [00:00<00:00, 6089.94it/s]\n"
     ]
    }
   ],
   "source": [
    "negative_sample_dics = {'random': val_negative_random, 'popularity': val_negative_popularity}\n",
    "res_list = []\n",
    "for train_sample_type in ['random', 'popularity']:\n",
    "    for validation_sample_type in ['random', 'popularity']:\n",
    "        print (f'Calculating measures on model trained on {train_sample_type} using {validation_sample_type} as validation set')\n",
    "        positive_samples = user_items_dict_val\n",
    "        negative_samples = negative_sample_dics[validation_sample_type]\n",
    "        user_emb = res_dic[f'{train_sample_type}_users']\n",
    "        item_emb = res_dic[f'{train_sample_type}_items']\n",
    "        # calculate metrics\n",
    "        mpr = MPR_calculation(positive_samples, negative_samples, user_emb, item_emb)\n",
    "        hr1 = Hit_Rate_at_k(positive_samples, negative_samples, user_emb, item_emb, 1)\n",
    "        hr10 = Hit_Rate_at_k(positive_samples, negative_samples, user_emb, item_emb, 10)\n",
    "        hr50 = Hit_Rate_at_k(positive_samples, negative_samples, user_emb, item_emb, 50)\n",
    "        validation_loss = validation_loss_func(positive_samples, negative_samples, user_emb, item_emb, 1e-4, 1e-4)\n",
    "        # add to results\n",
    "        res_list.append([train_sample_type, validation_sample_type, mpr, hr1, hr10, hr50, validation_loss])\n",
    "# save results to dataframe\n",
    "df_res = pd.DataFrame(res_list, columns=['train_sample_type', 'validation_sample_type', 'MPR', 'HR@1', 'HR@10', 'HR@50', 'validation_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_sample_type</th>\n",
       "      <th>validation_sample_type</th>\n",
       "      <th>MPR</th>\n",
       "      <th>HR@1</th>\n",
       "      <th>HR@10</th>\n",
       "      <th>HR@50</th>\n",
       "      <th>validation_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random</td>\n",
       "      <td>random</td>\n",
       "      <td>0.110508</td>\n",
       "      <td>0.116943</td>\n",
       "      <td>0.632726</td>\n",
       "      <td>0.971523</td>\n",
       "      <td>0.818106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random</td>\n",
       "      <td>popularity</td>\n",
       "      <td>0.110477</td>\n",
       "      <td>0.116280</td>\n",
       "      <td>0.634106</td>\n",
       "      <td>0.973068</td>\n",
       "      <td>0.817685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>popularity</td>\n",
       "      <td>random</td>\n",
       "      <td>0.110242</td>\n",
       "      <td>0.119592</td>\n",
       "      <td>0.635265</td>\n",
       "      <td>0.972461</td>\n",
       "      <td>0.818295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>popularity</td>\n",
       "      <td>popularity</td>\n",
       "      <td>0.110160</td>\n",
       "      <td>0.121689</td>\n",
       "      <td>0.635541</td>\n",
       "      <td>0.972848</td>\n",
       "      <td>0.817602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  train_sample_type validation_sample_type       MPR      HR@1     HR@10  \\\n",
       "0            random                 random  0.110508  0.116943  0.632726   \n",
       "1            random             popularity  0.110477  0.116280  0.634106   \n",
       "2        popularity                 random  0.110242  0.119592  0.635265   \n",
       "3        popularity             popularity  0.110160  0.121689  0.635541   \n",
       "\n",
       "      HR@50  validation_loss  \n",
       "0  0.971523         0.818106  \n",
       "1  0.973068         0.817685  \n",
       "2  0.972461         0.818295  \n",
       "3  0.972848         0.817602  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction On test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_on_test_set(row:pd.Series, users_embeddings:dict, items_embeddings:dict)->pd.Series:\n",
    "    user = row['UserID']\n",
    "    item_1 = row['Item1']\n",
    "    item_2 = row['Item2']\n",
    "    if item_1 not in items_embeddings.keys() or item_2 not in items_embeddings.keys():\n",
    "        return np.random.randint(0,2)\n",
    "        \n",
    "    item_1_score = np.dot(users_embeddings[user], items_embeddings[item_1])\n",
    "    item_2_score = np.dot(users_embeddings[user], items_embeddings[item_2])\n",
    "\n",
    "    if item_1_score > item_2_score:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set\n",
    "popularity_test = pd.read_csv('data/PopularityTest.csv')\n",
    "random_test = pd.read_csv('data/RandomTest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embeddings = res_dic['popularity_items']\n",
    "user_embeddings = res_dic['popularity_users']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity_test['bitClassification'] = popularity_test.apply(lambda row: prediction_on_test_set(row, user_embeddings, item_embeddings), axis=1)\n",
    "random_test['bitClassification'] = random_test.apply(lambda row: prediction_on_test_set(row, user_embeddings, item_embeddings), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f2772d500cfd5a3727f553e00f078c3ed1413cb482c87be9f8795da4fd24128"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
